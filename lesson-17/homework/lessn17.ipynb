{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q 1",
   "id": "e8b63150597e8423"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:02:00.973846Z",
     "start_time": "2025-05-07T13:01:56.611058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Connect to the Chinook database\n",
    "conn = sqlite3.connect('chinook.db')\n",
    "\n",
    "# Step 2: Define and execute the SQL query\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    c.CustomerId,\n",
    "    c.FirstName,\n",
    "    c.LastName,\n",
    "    COUNT(i.InvoiceId) AS TotalInvoices\n",
    "FROM\n",
    "    customers c\n",
    "INNER JOIN\n",
    "    invoices i ON c.CustomerId = i.CustomerId\n",
    "GROUP BY\n",
    "    c.CustomerId\n",
    "ORDER BY\n",
    "    TotalInvoices DESC;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Step 3: Display the results as a table\n",
    "print(df)\n",
    "\n",
    "# Step 4: Close the connection\n",
    "conn.close()\n"
   ],
   "id": "9e2f1c960e2946cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    CustomerId  FirstName      LastName  TotalInvoices\n",
      "0            1       Lu√≠s     Gon√ßalves              7\n",
      "1            2     Leonie        K√∂hler              7\n",
      "2            3   Fran√ßois      Tremblay              7\n",
      "3            4      Bj√∏rn        Hansen              7\n",
      "4            5  Franti≈°ek   Wichterlov√°              7\n",
      "5            6     Helena          Hol√Ω              7\n",
      "6            7     Astrid        Gruber              7\n",
      "7            8       Daan       Peeters              7\n",
      "8            9       Kara       Nielsen              7\n",
      "9           10    Eduardo       Martins              7\n",
      "10          11  Alexandre         Rocha              7\n",
      "11          12    Roberto       Almeida              7\n",
      "12          13   Fernanda         Ramos              7\n",
      "13          14       Mark       Philips              7\n",
      "14          15   Jennifer      Peterson              7\n",
      "15          16      Frank        Harris              7\n",
      "16          17       Jack         Smith              7\n",
      "17          18   Michelle        Brooks              7\n",
      "18          19        Tim         Goyer              7\n",
      "19          20        Dan        Miller              7\n",
      "20          21      Kathy         Chase              7\n",
      "21          22    Heather       Leacock              7\n",
      "22          23       John        Gordon              7\n",
      "23          24      Frank       Ralston              7\n",
      "24          25     Victor       Stevens              7\n",
      "25          26    Richard    Cunningham              7\n",
      "26          27    Patrick          Gray              7\n",
      "27          28      Julia       Barnett              7\n",
      "28          29     Robert         Brown              7\n",
      "29          30     Edward       Francis              7\n",
      "30          31     Martha          Silk              7\n",
      "31          32      Aaron      Mitchell              7\n",
      "32          33      Ellie      Sullivan              7\n",
      "33          34       Jo√£o     Fernandes              7\n",
      "34          35   Madalena       Sampaio              7\n",
      "35          36     Hannah     Schneider              7\n",
      "36          37       Fynn    Zimmermann              7\n",
      "37          38     Niklas      Schr√∂der              7\n",
      "38          39    Camille       Bernard              7\n",
      "39          40  Dominique      Lefebvre              7\n",
      "40          41       Marc        Dubois              7\n",
      "41          42      Wyatt        Girard              7\n",
      "42          43   Isabelle       Mercier              7\n",
      "43          44      Terhi    H√§m√§l√§inen              7\n",
      "44          45   Ladislav        Kov√°cs              7\n",
      "45          46       Hugh      O'Reilly              7\n",
      "46          47      Lucas       Mancini              7\n",
      "47          48   Johannes  Van der Berg              7\n",
      "48          49  Stanis≈Çaw        W√≥jcik              7\n",
      "49          50    Enrique         Mu√±oz              7\n",
      "50          51     Joakim     Johansson              7\n",
      "51          52       Emma         Jones              7\n",
      "52          53       Phil        Hughes              7\n",
      "53          54      Steve        Murray              7\n",
      "54          55       Mark        Taylor              7\n",
      "55          56      Diego     Guti√©rrez              7\n",
      "56          57       Luis         Rojas              7\n",
      "57          58      Manoj        Pareek              7\n",
      "58          59       Puja    Srivastava              6\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q 2",
   "id": "ef7ab56ffdec11d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:06:43.072959Z",
     "start_time": "2025-05-07T13:06:42.916514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "try:\n",
    "    movie_df = pd.read_csv('movie.csv')\n",
    "    print(\"‚úÖ movie.csv successfully loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: 'movie.csv' not found in the current directory.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Create smaller DataFrames\n",
    "df_director_color = movie_df[['director_name', 'color']].dropna(subset=['director_name'])\n",
    "df_director_reviews = movie_df[['director_name', 'num_critic_for_reviews']].dropna(subset=['director_name'])\n",
    "\n",
    "print(f\"\\nüé¨ df_director_color: {len(df_director_color)} rows\")\n",
    "print(f\"üìù df_director_reviews: {len(df_director_reviews)} rows\")\n",
    "\n",
    "# Step 3: Left Join on 'director_name'\n",
    "left_join_df = pd.merge(df_director_color, df_director_reviews, on='director_name', how='left')\n",
    "\n",
    "# Step 4: Full Outer Join on 'director_name'\n",
    "full_outer_join_df = pd.merge(df_director_color, df_director_reviews, on='director_name', how='outer')\n",
    "\n",
    "# Step 5: Report the number of rows in each resulting DataFrame\n",
    "print(f\"\\nüîó Left Join Result: {len(left_join_df)} rows\")\n",
    "print(f\"üåê Full Outer Join Result: {len(full_outer_join_df)} rows\")\n",
    "\n",
    "# Step 6: Optional ‚Äî Show first 5 rows for each join\n",
    "print(\"\\nüìã Sample of Left Join Result:\")\n",
    "print(left_join_df.head())\n",
    "\n",
    "print(\"\\nüìã Sample of Full Outer Join Result:\")\n",
    "print(full_outer_join_df.head())\n"
   ],
   "id": "6026ba22b6ddcc17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ movie.csv successfully loaded.\n",
      "\n",
      "üé¨ df_director_color: 4814 rows\n",
      "üìù df_director_reviews: 4814 rows\n",
      "\n",
      "üîó Left Join Result: 19896 rows\n",
      "üåê Full Outer Join Result: 19896 rows\n",
      "\n",
      "üìã Sample of Left Join Result:\n",
      "   director_name  color  num_critic_for_reviews\n",
      "0  James Cameron  Color                   723.0\n",
      "1  James Cameron  Color                   315.0\n",
      "2  James Cameron  Color                   210.0\n",
      "3  James Cameron  Color                    94.0\n",
      "4  James Cameron  Color                    82.0\n",
      "\n",
      "üìã Sample of Full Outer Join Result:\n",
      "     director_name  color  num_critic_for_reviews\n",
      "0    A. Raven Cruz  Color                     3.0\n",
      "1       Aaron Hann  Color                    29.0\n",
      "2  Aaron Schneider  Color                   160.0\n",
      "3    Aaron Seltzer  Color                    99.0\n",
      "4     Abel Ferrara  Color                    48.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Grouping and Aggregating Q 1",
   "id": "e11e8395aec04949"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:17:26.498257Z",
     "start_time": "2025-05-07T13:17:26.300002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the Titanic dataset from Excel\n",
    "try:\n",
    "    titanic_df = pd.read_excel('titanic.xlsx')\n",
    "    print(\"‚úÖ titanic.xlsx successfully loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: 'titanic.xlsx' not found in the current directory.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Group by 'Pclass' and perform aggregations\n",
    "grouped_df = titanic_df.groupby('Pclass').agg(\n",
    "    Average_Age=('Age', 'mean'),\n",
    "    Total_Fare=('Fare', 'sum'),\n",
    "    Passenger_Count=('PassengerId', 'count')  # Assumes 'PassengerId' exists\n",
    ").reset_index()\n",
    "\n",
    "# Step 3: Display the resulting DataFrame\n",
    "print(\"\\nüìä Grouped Aggregations by Pclass:\")\n",
    "print(grouped_df)\n"
   ],
   "id": "3df306f2592f0145",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ titanic.xlsx successfully loaded.\n",
      "\n",
      "üìä Grouped Aggregations by Pclass:\n",
      "   Pclass  Average_Age  Total_Fare  Passenger_Count\n",
      "0       1    38.233441  18177.4125              216\n",
      "1       2    29.877630   3801.8417              184\n",
      "2       3    25.140620   6714.6951              491\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q 2",
   "id": "28a4e127c2a09e82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:18:51.703722Z",
     "start_time": "2025-05-07T13:18:51.602928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the movie dataset (Excel or CSV as needed)\n",
    "try:\n",
    "    movie_df = pd.read_csv('movie.csv')  # or use pd.read_excel('movie.xlsx') if your file is .xlsx\n",
    "    print(\"‚úÖ movie.csv successfully loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: 'movie.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Drop rows where 'color' or 'director_name' is missing\n",
    "movie_df = movie_df.dropna(subset=['color', 'director_name'])\n",
    "\n",
    "# Step 3: Perform multi-level grouping and aggregation\n",
    "grouped_df = movie_df.groupby(['color', 'director_name']).agg(\n",
    "    Total_Critic_Reviews=('num_critic_for_reviews', 'sum'),\n",
    "    Average_Duration=('duration', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Step 4: Display the result\n",
    "print(\"\\nüé¨ Multi-level Grouping by Color and Director:\")\n",
    "print(grouped_df.head())\n"
   ],
   "id": "80286a15bcf9a146",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ movie.csv successfully loaded.\n",
      "\n",
      "üé¨ Multi-level Grouping by Color and Director:\n",
      "             color     director_name  Total_Critic_Reviews  Average_Duration\n",
      "0  Black and White    Akira Kurosawa                 153.0             202.0\n",
      "1  Black and White    Aleksey German                 121.0             177.0\n",
      "2  Black and White      Alex Garland                 489.0             108.0\n",
      "3  Black and White   Alexander Payne                 433.0             115.0\n",
      "4  Black and White  Alfred Hitchcock                 434.0             119.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q 3",
   "id": "9f70917bdebdd4df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:35:30.929678Z",
     "start_time": "2025-05-07T13:35:30.890950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data (replace this with your dataset)\n",
    "data = {\n",
    "    'Year': [2023, 2023, 2023, 2024, 2024],\n",
    "    'Month': [1, 1, 2, 2, 3],\n",
    "    'ArrDelay': [15, -5, 30, 10, 0],\n",
    "    'DepDelay': [10, 5, 25, 20, 15],\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Group by Year and Month, then calculate required metrics\n",
    "result = df.groupby(['Year', 'Month']).agg(\n",
    "    Total_Flights=('ArrDelay', 'size'),\n",
    "    Avg_Arrival_Delay=('ArrDelay', 'mean'),\n",
    "    Max_Departure_Delay=('DepDelay', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Display the result\n",
    "print(result)\n"
   ],
   "id": "754c2d2102b53d71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Month  Total_Flights  Avg_Arrival_Delay  Max_Departure_Delay\n",
      "0  2023      1              2                5.0                   10\n",
      "1  2023      2              1               30.0                   25\n",
      "2  2024      2              1               10.0                   20\n",
      "3  2024      3              1                0.0                   15\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Applying Functions Q 1",
   "id": "5cdf24490eac310b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:39:33.857013Z",
     "start_time": "2025-05-07T13:39:33.038712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset from the Excel file\n",
    "df = pd.read_excel('titanic.xlsx')\n",
    "\n",
    "# Custom function to classify passengers as Child or Adult\n",
    "def classify_age_group(age):\n",
    "    if age < 18:\n",
    "        return 'Child'\n",
    "    else:\n",
    "        return 'Adult'\n",
    "\n",
    "# Apply the function to create a new column 'Age_Group'\n",
    "df['Age_Group'] = df['Age'].apply(classify_age_group)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(df[['Name', 'Age', 'Age_Group']].head())\n"
   ],
   "id": "c53f3de9bbd8b91e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Name   Age Age_Group\n",
      "0                            Braund, Mr. Owen Harris  22.0     Adult\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0     Adult\n",
      "2                             Heikkinen, Miss. Laina  26.0     Adult\n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0     Adult\n",
      "4                           Allen, Mr. William Henry  35.0     Adult\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q 2",
   "id": "1a793c7791fe2357"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:46:29.182808Z",
     "start_time": "2025-05-07T13:46:26.631236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the employee data\n",
    "df = pd.read_csv(\"employee.csv\")\n",
    "\n",
    "# Preview the data\n",
    "print(df.head())\n",
    "\n",
    "# Min-Max Normalization per Department\n",
    "df['NormalizedSalary'] = df.groupby('Department')['Salary'].transform(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min())\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(df[['EmployeeID', 'Department', 'Salary', 'NormalizedSalary']])\n",
    "\n",
    "# Optional: Save to new CSV\n",
    "df.to_csv(\"normalized_employee_salaries.csv\", index=False)\n"
   ],
   "id": "d3eb502dc21aef3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UNIQUE_ID               POSITION_TITLE                     DEPARTMENT  \\\n",
      "0          0  ASSISTANT DIRECTOR (EX LVL)    Municipal Courts Department   \n",
      "1          1            LIBRARY ASSISTANT                        Library   \n",
      "2          2               POLICE OFFICER  Houston Police Department-HPD   \n",
      "3          3            ENGINEER/OPERATOR  Houston Fire Department (HFD)   \n",
      "4          4                  ELECTRICIAN    General Services Department   \n",
      "\n",
      "   BASE_SALARY             RACE EMPLOYMENT_TYPE  GENDER EMPLOYMENT_STATUS  \\\n",
      "0     121862.0  Hispanic/Latino       Full Time  Female            Active   \n",
      "1      26125.0  Hispanic/Latino       Full Time  Female            Active   \n",
      "2      45279.0            White       Full Time    Male            Active   \n",
      "3      63166.0            White       Full Time    Male            Active   \n",
      "4      56347.0            White       Full Time    Male            Active   \n",
      "\n",
      "    HIRE_DATE    JOB_DATE  \n",
      "0  2006-06-12  2012-10-13  \n",
      "1  2000-07-19  2010-09-18  \n",
      "2  2015-02-03  2015-02-03  \n",
      "3  1982-02-08  1991-05-25  \n",
      "4  1989-06-19  1994-10-22  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Department'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(df.head())\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# Min-Max Normalization per Department\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mNormalizedSalary\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mDepartment\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[33m'\u001B[39m\u001B[33mSalary\u001B[39m\u001B[33m'\u001B[39m].transform(\n\u001B[32m     11\u001B[39m     \u001B[38;5;28;01mlambda\u001B[39;00m x: (x - x.min()) / (x.max() - x.min())\n\u001B[32m     12\u001B[39m )\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Display the result\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;28mprint\u001B[39m(df[[\u001B[33m'\u001B[39m\u001B[33mEmployeeID\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mDepartment\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mSalary\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mNormalizedSalary\u001B[39m\u001B[33m'\u001B[39m]])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Data Science\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:9183\u001B[39m, in \u001B[36mDataFrame.groupby\u001B[39m\u001B[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001B[39m\n\u001B[32m   9180\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m level \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m by \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   9181\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mYou have to supply one of \u001B[39m\u001B[33m'\u001B[39m\u001B[33mby\u001B[39m\u001B[33m'\u001B[39m\u001B[33m and \u001B[39m\u001B[33m'\u001B[39m\u001B[33mlevel\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m9183\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameGroupBy\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   9184\u001B[39m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   9185\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m=\u001B[49m\u001B[43mby\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   9186\u001B[39m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   9187\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   9188\u001B[39m \u001B[43m    \u001B[49m\u001B[43mas_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mas_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   9189\u001B[39m \u001B[43m    \u001B[49m\u001B[43msort\u001B[49m\u001B[43m=\u001B[49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   9190\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgroup_keys\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   9191\u001B[39m \u001B[43m    \u001B[49m\u001B[43mobserved\u001B[49m\u001B[43m=\u001B[49m\u001B[43mobserved\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   9192\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdropna\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdropna\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   9193\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Data Science\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001B[39m, in \u001B[36mGroupBy.__init__\u001B[39m\u001B[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001B[39m\n\u001B[32m   1326\u001B[39m \u001B[38;5;28mself\u001B[39m.dropna = dropna\n\u001B[32m   1328\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m grouper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1329\u001B[39m     grouper, exclusions, obj = \u001B[43mget_grouper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1330\u001B[39m \u001B[43m        \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1331\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1332\u001B[39m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1333\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1334\u001B[39m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[43m=\u001B[49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1335\u001B[39m \u001B[43m        \u001B[49m\u001B[43mobserved\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobserved\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mno_default\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobserved\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1336\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdropna\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropna\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1337\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1339\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m observed \u001B[38;5;129;01mis\u001B[39;00m lib.no_default:\n\u001B[32m   1340\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(ping._passed_categorical \u001B[38;5;28;01mfor\u001B[39;00m ping \u001B[38;5;129;01min\u001B[39;00m grouper.groupings):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Data Science\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001B[39m, in \u001B[36mget_grouper\u001B[39m\u001B[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001B[39m\n\u001B[32m   1041\u001B[39m         in_axis, level, gpr = \u001B[38;5;28;01mFalse\u001B[39;00m, gpr, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1042\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1043\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(gpr)\n\u001B[32m   1044\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(gpr, Grouper) \u001B[38;5;129;01mand\u001B[39;00m gpr.key \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1045\u001B[39m     \u001B[38;5;66;03m# Add key to exclusions\u001B[39;00m\n\u001B[32m   1046\u001B[39m     exclusions.add(gpr.key)\n",
      "\u001B[31mKeyError\u001B[39m: 'Department'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q 3",
   "id": "258d0cb548df9fae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:49:38.802282Z",
     "start_time": "2025-05-07T13:49:38.723645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the movie dataset\n",
    "df = pd.read_csv('movie.csv')\n",
    "\n",
    "# Define the custom function to classify movie duration\n",
    "def classify_duration(duration):\n",
    "    if duration < 60:\n",
    "        return 'Short'\n",
    "    elif 60 <= duration <= 120:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Long'\n",
    "\n",
    "# Apply the function to create a new column 'Duration_Type'\n",
    "df['Duration_Type'] = df['duration'].apply(classify_duration)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(df[['movie_title', 'duration', 'Duration_Type']].head())\n"
   ],
   "id": "3fb82e7cd176cdc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  movie_title  duration Duration_Type\n",
      "0                                      Avatar     178.0          Long\n",
      "1    Pirates of the Caribbean: At World's End     169.0          Long\n",
      "2                                     Spectre     148.0          Long\n",
      "3                       The Dark Knight Rises     164.0          Long\n",
      "4  Star Wars: Episode VII - The Force Awakens       NaN          Long\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using pipe Q 1",
   "id": "b1ec498c680c1412"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:51:07.160198Z",
     "start_time": "2025-05-07T13:51:06.207797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "df = pd.read_excel('titanic.xlsx')\n",
    "\n",
    "# Step 1: Filter passengers who survived (Survived == 1)\n",
    "df = df[df['Survived'] == 1]\n",
    "\n",
    "# Step 2: Fill missing Age values with the mean\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "\n",
    "# Step 3: Create a new column 'Fare_Per_Age' by dividing Fare by Age\n",
    "df['Fare_Per_Age'] = df['Fare'] / df['Age']\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(df[['Name', 'Age', 'Fare', 'Fare_Per_Age']].head())\n"
   ],
   "id": "aaf031277d085f9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Name   Age     Fare  \\\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0  71.2833   \n",
      "2                             Heikkinen, Miss. Laina  26.0   7.9250   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0  53.1000   \n",
      "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  27.0  11.1333   \n",
      "9                Nasser, Mrs. Nicholas (Adele Achem)  14.0  30.0708   \n",
      "\n",
      "   Fare_Per_Age  \n",
      "1      1.875876  \n",
      "2      0.304808  \n",
      "3      1.517143  \n",
      "8      0.412344  \n",
      "9      2.147914  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q 2\n",
   "id": "a4801ee6e8f0c795"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:52:37.321980Z",
     "start_time": "2025-05-07T13:52:37.296566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a mock flights dataset\n",
    "data = {\n",
    "    'FlightID': [101, 102, 103, 104, 105],\n",
    "    'DepartureDelay': [45, 20, 35, 50, 25],\n",
    "    'ScheduledFlightDuration': [120, 150, 90, 180, 120]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Filter flights with a departure delay greater than 30 minutes\n",
    "df = df[df['DepartureDelay'] > 30]\n",
    "\n",
    "# Step 2: Add a new column 'Delay_Per_Hour' by dividing the delay by the scheduled flight duration\n",
    "df['Delay_Per_Hour'] = df['DepartureDelay'] / df['ScheduledFlightDuration']\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(df[['FlightID', 'DepartureDelay', 'ScheduledFlightDuration', 'Delay_Per_Hour']])\n"
   ],
   "id": "17973218c26b0a9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FlightID  DepartureDelay  ScheduledFlightDuration  Delay_Per_Hour\n",
      "0       101              45                      120        0.375000\n",
      "2       103              35                       90        0.388889\n",
      "3       104              50                      180        0.277778\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
